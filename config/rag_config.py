import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from typing import Optional
import torch
from langchain_community.embeddings import ZhipuAIEmbeddings
from zhipuai import ZhipuAI
from config.path_config import BGE_RERANKER_MODEL

load_dotenv()
# rerank_url - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
rerank_url = BGE_RERANKER_MODEL
# db_url
db_url = 'mysql+aiomysql://shuke:123456@localhost/sk_db?charset=utf8mb4'
_rerank_model = None
client = ZhipuAI(api_key=os.getenv('ZHIPUAI_API_KEY'))
llm = ChatOpenAI(
    model_name=os.getenv('ZHIPU_MODEL_NAME'),
    api_key=os.getenv('ZHIPUAI_API_KEY'),
    base_url=os.getenv('ZHIPUAI_URL')
)
ZHIPUEmbeddings =ZhipuAIEmbeddings(
            model_name='embedding-2',
            api_key=os.getenv('ZHIPUAI_API_KEY')
        )


class ZhipuReranker:

    def __init__(self, client: ZhipuAI, model: str = "glm-4-flash", top_n: int = 10):

        self.client = client
        self.model = model
        self.top_n = top_n

    def rerank(self, query: str, documents: list, top_n: Optional[int] = None) -> list:

        from langchain_core.documents import Document

        if not documents:
            return []

        # åˆ¤æ–­è¾“å…¥æ˜¯ Document å¯¹è±¡è¿˜æ˜¯å­—ç¬¦ä¸²
        is_document_list = isinstance(documents[0], Document)

        # æå–æ–‡æœ¬å†…å®¹
        if is_document_list:
            texts = [doc.page_content for doc in documents]
        else:
            texts = documents

        # è°ƒç”¨æ™ºè°± rerank API
        try:
            response = self.client.rerank(
                model=self.model,
                query=query,
                documents=texts,
                top_n=top_n or self.top_n
            )

            # è§£æè¿”å›ç»“æœ
            reranked_results = []
            for item in response.results:
                idx = item.index  # åŸå§‹æ–‡æ¡£çš„ç´¢å¼•
                score = item.relevance_score  # ç›¸å…³æ€§åˆ†æ•°

                # æ„é€ è¿”å›çš„ Document å¯¹è±¡
                if is_document_list:
                    doc = documents[idx]
                    # æ·»åŠ  rerank åˆ†æ•°åˆ° metadata
                    doc.metadata['rerank_score'] = score
                    reranked_results.append(doc)
                else:
                    # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ›å»ºæ–°çš„ Document å¯¹è±¡
                    doc = Document(
                        page_content=texts[idx],
                        metadata={'rerank_score': score}
                    )
                    reranked_results.append(doc)

            return reranked_results

        except Exception as e:
            print(f"âŒ æ™ºè°± Rerank è°ƒç”¨å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ–‡æ¡£
            if is_document_list:
                return documents[:top_n or self.top_n]
            else:
                return [Document(page_content=text) for text in texts[:top_n or self.top_n]]

    def invoke(self, query: str, documents: list = None, top_n: Optional[int] = None):
        """
        å…¼å®¹ LangChain çš„ invoke è°ƒç”¨æ–¹å¼
        """
        if documents is None:
            documents = []
        return self.rerank(query, documents, top_n)


# åˆ›å»ºå…¨å±€ reranker å®ä¾‹
zhipu_reranker = ZhipuReranker(client=client, top_n=10)
#db_setting (moved to top)

#get_device
def get_device():
    """
    è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    """
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"ğŸš€ æ£€æµ‹åˆ° NVIDIA GPUï¼Œå°†ä½¿ç”¨ CUDA åŠ é€Ÿ")
    elif torch.backends.mps.is_available():
        device = 'mps'
        print(f"ğŸš€ æ£€æµ‹åˆ° Apple Siliconï¼Œå°†ä½¿ç”¨ MPS åŠ é€Ÿ")
    else:
        device = 'cpu'
        print(f"ğŸ’» å°†ä½¿ç”¨ CPU è¿è¡Œ")
    return device


if __name__ == '__main__':
    print(llm.invoke('ä½ å¥½'))

