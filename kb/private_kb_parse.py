from config.path_config import (
    PROJECT_ROOT, KB_LIST_DIR, KB_DIR, KB_SAVE_PATH_DIR, KB_PARSE_RESULT_DIR,
    PRIVATE_KB_DIR, PRIVATE_KB_VECTOR, DATA_DIR, RAW_DATA_DIR, DOCUMENTS_DIR,
    EVALUATION_DIR, EVALUATION_RESULTS_DIR, TEST_DATASET_PATH,
    VECTOR_STORE_DIR, DB_DIR, BGE_RERANKER_MODEL
)
"""
ç§æœ‰çŸ¥è¯†åº“è§£æžå™¨ - æ–¹æ¡ˆ2ï¼šJSONL ä¿å­˜å®Œæ•´ metadata

åŠŸèƒ½ï¼š
1. è§£æž Markdown æ–‡ä»¶ï¼Œæå–æ ‡é¢˜å’Œç« èŠ‚
2. ç”Ÿæˆ Document å¯¹è±¡
3. ä¿å­˜ä¸º JSONL æ ¼å¼ï¼ˆåŒ…å«å®Œæ•´ metadataï¼‰
4. å…¼å®¹ add_chunk2vector æ–¹æ³•
"""
import warnings

warnings.filterwarnings('ignore', category=UserWarning,
                        module='pkg_resources')
from langchain_community.document_loaders import UnstructuredMarkdownLoader, Docx2txtLoader
import re
import json
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from typing import List, Tuple, Dict

def _extract_sections(file_path: Path) -> Tuple[List[Dict], str]:

    if not file_path.is_file():
        raise ValueError(f"è·¯å¾„å¿…é¡»æ˜¯æ–‡ä»¶: {file_path}")

    if file_path.suffix != '.md':
        raise ValueError(f"åªæ”¯æŒ .md æ–‡ä»¶: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    sections = []
    lines = content.split('\n')
    current_title = None
    current_content = []
    doc_title = None

    for line in lines:
        # æ£€æµ‹ä¸»æ ‡é¢˜ï¼ˆä¸€çº§æ ‡é¢˜ï¼š# æ ‡é¢˜ï¼‰
        main_match = re.match(r'^#\s+(.+)$', line)
        if main_match and doc_title is None:
            doc_title = main_match.group(1).strip()
            continue

        # æ£€æµ‹ç« èŠ‚æ ‡é¢˜ï¼ˆäºŒçº§æ ‡é¢˜ï¼š## æ ‡é¢˜ï¼‰
        section_match = re.match(r'^##\s+(.+)$', line)
        if section_match:
            if current_title:
                content_text = '\n'.join(current_content).strip()
                if content_text:
                    sections.append({
                        'title': current_title,
                        'content': content_text
                    })
            current_title = section_match.group(1).strip()
            current_content = []
        else:
            if current_title:
                current_content.append(line)

    if current_title:
        content_text = '\n'.join(current_content).strip()
        if content_text:
            sections.append({
                'title': current_title,
                'content': content_text
            })

    # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ä¸»æ ‡é¢˜ï¼Œä½¿ç”¨æ–‡ä»¶å
    if doc_title is None:
        doc_title = file_path.stem

    return sections, doc_title

def _extract_word_sections(file_path: Path) -> Tuple[List[Dict], str]:
    file_path = Path(file_path)
    if file_path.suffix not in ['.docx', '.doc']:
        raise ValueError(f"åªæ”¯æŒ .docx æˆ– .doc æ–‡ä»¶: {file_path}")

    loader = Docx2txtLoader(file_path)
    docs = loader.load()

    if not docs:
        return [], file_path.stem

    content = docs[0].page_content

    lines = content.split('\n')

    sections = []
    doc_title = None
    current_title = None
    current_content = []

    for line in lines:
        line = line.strip()

        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue

        if doc_title is None:
            if not re.match(r'^\d+[.ã€]', line) and not re.match(r'^[#]+\s+', line):
                doc_title = line
                continue
            else:
                doc_title = file_path.stem

        # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
        is_section_title = False

        # æ¨¡å¼1: "1. xxx", "2. xxx" ç­‰ï¼ˆå•å±‚ç¼–å·ï¼‰
        if re.match(r'^\d+\.\s+', line):
            is_section_title = True
        # æ¨¡å¼2: "1.1 xxx", "2.3 xxx" ç­‰ï¼ˆä¸¤å±‚ç¼–å·ï¼‰
        elif re.match(r'^\d+\.\d+\s+', line):
            is_section_title = True
        # æ¨¡å¼3: "## " å¼€å¤´
        elif re.match(r'^##\s+', line):
            is_section_title = True
        # æ¨¡å¼4: "ï¼ˆä¸€ï¼‰", "ï¼ˆäºŒï¼‰" ç­‰
        elif re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰)]\s*', line):
            is_section_title = True
        # æ¨¡å¼5: "ä¸€ã€", "äºŒã€" ç­‰
        elif re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€]\s*', line):
            is_section_title = True

        if is_section_title:
            # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
            if current_title:
                content_text = '\n'.join(current_content).strip()
                if content_text:
                    sections.append({
                        'title': current_title,
                        'content': content_text
                    })

            current_title = re.sub(r'^[\d#ï¼ˆ(ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[.ã€\sï¼‰)]+', '', line).strip()
            if not current_title:
                current_title = line
            current_content = []
        else:

            if current_title:
                current_content.append(line)

    # ä¿å­˜æœ€åŽä¸€ä¸ªç« èŠ‚
    if current_title:
        content_text = '\n'.join(current_content).strip()
        if content_text:
            sections.append({
                'title': current_title,
                'content': content_text
            })

    if doc_title is None:
        doc_title = file_path.stem

    return sections, doc_title



def parse(folder_path: str, kb_type: str = 'private', category: str = None,
          chunk_size: int = 500, chunk_overlap: int = 50) -> List[Document]:

    folder_path = Path(folder_path)

    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not folder_path.exists():
        print('è·¯å¾„ä¸å­˜åœ¨')
    # else:
    #     folder_path.mkdir(parents=True, exist_ok=True)

    # èŽ·å–æ‰€æœ‰ .md å’Œ .docx æ–‡ä»¶
    all_files = []
    if folder_path.is_dir():
        md_files = list(folder_path.glob('*.md'))
        docx_files = list(folder_path.glob('*.docx'))
        all_files = md_files + docx_files

        if not all_files:
            print(f'âš ï¸ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° .md æˆ– .docx æ–‡ä»¶: {folder_path}')
            return []
        print(f'ðŸ“ åœ¨æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(md_files)} ä¸ª .md æ–‡ä»¶, {len(docx_files)} ä¸ª .docx æ–‡ä»¶')
    elif folder_path.is_file() and folder_path.suffix in ['.md', '.docx']:

        all_files = [folder_path]
        print(f'ðŸ“„ æ£€æµ‹åˆ°å•ä¸ª {folder_path.suffix} æ–‡ä»¶')
    else:
        print(f'âš ï¸ ä¸æ˜¯æ–‡ä»¶å¤¹æˆ–æ”¯æŒçš„æ–‡ä»¶ç±»åž‹: {folder_path}')
        return []

    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )

    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    all_docs = []

    for file in all_files:
        print(f'\nðŸ“– å¼€å§‹è§£æžæ–‡ä»¶: {file.name}')

        try:
            # æ ¹æ®æ–‡ä»¶ç±»åž‹é€‰æ‹©è§£æžå‡½æ•°
            if file.suffix == '.md':
                sections, doc_title = _extract_sections(file)
            elif file.suffix == '.docx':
                sections, doc_title = _extract_word_sections(file)
            else:
                print(f'âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»åž‹: {file.suffix}')
                continue

            if not sections:
                print(f'âš ï¸ æ–‡ä»¶ {file.name} æœªæå–åˆ°ä»»ä½•ç« èŠ‚ï¼Œè·³è¿‡')
                continue

            print(f'ðŸ“š æ–‡æ¡£æ ‡é¢˜: {doc_title}')
            print(f'ðŸ“‘ æå–åˆ° {len(sections)} ä¸ªç« èŠ‚')

            # 2. èŽ·å–æ–‡ä»¶åå’Œåˆ†ç±»
            file_name = file.stem
            current_category = category if category else doc_title

            # 3. å¤„ç†æ¯ä¸ªç« èŠ‚
            for section_idx, section in enumerate(sections):
                # åˆ›å»ºåˆå§‹ Document
                doc = Document(
                    page_content=section['content'],
                    metadata={
                        'doc_title': doc_title,
                        'section_title': section['title'],
                        'source': str(file),
                        'file_name': file_name,
                        'kb_type': kb_type,
                        'category': current_category,
                        'section_index': section_idx
                    }
                )

                # å¦‚æžœç« èŠ‚å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ‡åˆ†
                if len(section['content']) > chunk_size:
                    print(f"  âœ‚ï¸  ç« èŠ‚ '{section['title']}' è¾ƒé•¿ ({len(section['content'])} å­—ç¬¦)ï¼Œåˆ‡åˆ†ä¸­...")
                    chunks = text_splitter.split_documents([doc])
                else:
                    chunks = [doc]

                # ä¸ºæ¯ä¸ª chunk æ·»åŠ è¯¦ç»†çš„ metadata
                for chunk_idx, chunk in enumerate(chunks):
                    chunk.metadata.update({
                        'id': f"{file_name}_s{section_idx}_c{chunk_idx}",
                        'title': section['title'],  # ç« èŠ‚æ ‡é¢˜ä½œä¸º title
                        'chunk_index_in_section': chunk_idx,
                        'total_chunks_in_section': len(chunks)
                    })
                    all_docs.append(chunk)

            print(f'âœ… {file.name} è§£æžå®Œæˆ')

        except Exception as e:
            print(f'âŒ å¤„ç†æ–‡ä»¶ {file.name} å¤±è´¥: {e}')
            continue

    for global_idx, doc in enumerate(all_docs):
        doc.metadata['global_chunk_index'] = global_idx
        doc.metadata['total_chunks'] = len(all_docs)

    print(f'\nâœ… æ€»è®¡è§£æžå®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_docs)} ä¸ªæ–‡æ¡£å—\n')

    return all_docs


def save_to_jsonl_with_full_metadata(docs: List[Document], output_path: str):

    with open(output_path, 'w', encoding='utf-8') as f:
        for doc in docs:
            item = {
                'id': doc.metadata['id'],
                'title': doc.metadata['title'],
                'contents': doc.page_content,
                'metadata': {
                    'doc_title': doc.metadata.get('doc_title'),
                    'section_title': doc.metadata.get('section_title'),
                    'source': doc.metadata.get('source'),
                    'file_name': doc.metadata.get('file_name'),
                    'kb_type': doc.metadata.get('kb_type'),
                    'category': doc.metadata.get('category'),
                    'section_index': doc.metadata.get('section_index'),
                    'chunk_index_in_section': doc.metadata.get('chunk_index_in_section'),
                    'total_chunks_in_section': doc.metadata.get('total_chunks_in_section'),
                    'global_chunk_index': doc.metadata.get('global_chunk_index'),
                    'total_chunks': doc.metadata.get('total_chunks')
                }
            }
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f'ðŸ’¾ å·²ä¿å­˜ {len(docs)} ä¸ªæ–‡æ¡£å—ï¼ˆå«å®Œæ•´metadataï¼‰åˆ°: {output_path}')



if __name__ == '__main__':
    folder_path = str(DOCUMENTS_DIR)

    # è§£æžæ–‡æ¡£
    # docs = parse(
    #     folder_path,
    #     kb_type='private',
    #     category='python_tutorial',
    #     chunk_size=500,
    #     chunk_overlap=50
    # )

    output_path = str(KB_SAVE_PATH_DIR / 'python_chunk_250.jsonl')
    #save_to_jsonl_with_full_metadata(docs, output_path)
    # def parse(folder_path: str, kb_type: str = 'private', category: str = None,
    #           chunk_size: int = 500, chunk_overlap: int = 50) -> List[Document]:
    folder_path = str(DOCUMENTS_DIR)
    docs = parse(folder_path=folder_path,kb_type='private',chunk_size=250,chunk_overlap=50)
    save_to_jsonl_with_full_metadata(docs, output_path)