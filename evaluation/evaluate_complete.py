"""
å®Œæ•´RAGç³»ç»Ÿè¯„ä¼°è„šæœ¬
æ•´åˆäº†ï¼šæ£€ç´¢è¯„ä¼°ã€ç”Ÿæˆè¯„ä¼°(ragas)ã€ç«¯åˆ°ç«¯è¯„ä¼°
"""

import json
import time
from typing import List, Dict, Any
from pathlib import Path

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall

# ========================================
# ç¬¬ä¸€éƒ¨åˆ†: æ£€ç´¢è¯„ä¼°æŒ‡æ ‡
# ========================================

class RetrievalMetrics:
    """è®¡ç®—æ£€ç´¢ç›¸å…³çš„æŒ‡æ ‡"""

    @staticmethod
    def recall_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
        """
        Recall@K: å‰Kä¸ªæ£€ç´¢ç»“æœä¸­,æœ‰å¤šå°‘ç›¸å…³æ–‡æ¡£è¢«å¬å›
        å…¬å¼: Recall@K = |æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£| / |æ‰€æœ‰ç›¸å…³æ–‡æ¡£|
        """
        if not relevant_ids:
            return 0.0

        retrieved_set = set(retrieved_ids[:k])
        relevant_set = set(relevant_ids)
        hits = len(retrieved_set & relevant_set)

        return hits / len(relevant_set)

    @staticmethod
    def precision_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
        """
        Precision@K: å‰Kä¸ªæ£€ç´¢ç»“æœä¸­,æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„
        å…¬å¼: Precision@K = |æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£| / K
        """
        if k == 0:
            return 0.0

        retrieved_set = set(retrieved_ids[:k])
        relevant_set = set(relevant_ids)
        hits = len(retrieved_set & relevant_set)

        return hits / k

    @staticmethod
    def mrr(retrieved_ids: List[str], relevant_ids: List[str]) -> float:
        """
        MRR (Mean Reciprocal Rank): ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æ’åå€’æ•°
        å…¬å¼: MRR = 1 / rank(ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£)
        """
        relevant_set = set(relevant_ids)

        for i, doc_id in enumerate(retrieved_ids, 1):
            if doc_id in relevant_set:
                return 1.0 / i

        return 0.0


# ========================================
# ç¬¬äºŒéƒ¨åˆ†: å®Œæ•´è¯„ä¼°å™¨
# ========================================

class CompleteEvaluator:
    """æ•´åˆæ£€ç´¢ã€ç”Ÿæˆã€ç«¯åˆ°ç«¯çš„å®Œæ•´è¯„ä¼°å™¨"""

    def __init__(
        self,
        vector_store_path: str,
        embedding_model_name: str = "BAAI/bge-small-zh-v1.5"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            vector_store_path: FAISSå‘é‡åº“è·¯å¾„
            embedding_model_name: embeddingæ¨¡å‹åç§°
        """
        print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")

        # åŠ è½½embeddingæ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': 'cpu'}
        )

        # åŠ è½½å‘é‡åº“
        self.vector_store = FAISS.load_local(
            vector_store_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )

        print(f"âœ… å‘é‡åº“åŠ è½½å®Œæˆ: {vector_store_path}")

    def retrieve_documents(self, query: str, k: int = 10) -> tuple:
        """
        æ£€ç´¢æ–‡æ¡£å¹¶è¿”å›æ–‡æ¡£IDå’Œå†…å®¹

        Returns:
            (retrieved_ids, contexts): æ–‡æ¡£IDåˆ—è¡¨å’Œå†…å®¹åˆ—è¡¨
        """
        # æ£€ç´¢æ–‡æ¡£
        docs = self.vector_store.similarity_search(query, k=k)

        # âœ… ä»metadataæå–æ–‡æ¡£ID
        retrieved_ids = [
            doc.metadata.get('id', '')
            for doc in docs
            if doc.metadata.get('id')
        ]

        # æå–æ–‡æ¡£å†…å®¹ï¼ˆç”¨äºragasè¯„ä¼°ï¼‰
        contexts = [doc.page_content for doc in docs]

        return retrieved_ids, contexts

    def compute_retrieval_metrics(
        self,
        retrieved_ids: List[str],
        relevant_docs: List[str],
        k_values: List[int]
    ) -> Dict[str, float]:
        """
        æ ¹æ®æ£€ç´¢ç»“æœè®¡ç®—å„é¡¹æŒ‡æ ‡

        Args:
            retrieved_ids: å½“å‰æ£€ç´¢å¾—åˆ°çš„æ–‡æ¡£ID
            relevant_docs: æ ‡æ³¨çš„ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            k_values: è¦è¯„ä¼°çš„Kå€¼åˆ—è¡¨

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        for k in k_values:
            metrics[f'recall@{k}'] = RetrievalMetrics.recall_at_k(
                retrieved_ids, relevant_docs, k
            )
            metrics[f'precision@{k}'] = RetrievalMetrics.precision_at_k(
                retrieved_ids, relevant_docs, k
            )

        metrics['mrr'] = RetrievalMetrics.mrr(retrieved_ids, relevant_docs)
        return metrics

    def evaluate_retrieval(
        self,
        query: str,
        relevant_docs: List[str],
        k_values: List[int] = [3, 5, 10]
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ£€ç´¢æ€§èƒ½

        Args:
            query: æŸ¥è¯¢é—®é¢˜
            relevant_docs: æ ‡æ³¨çš„ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            k_values: è¦è¯„ä¼°çš„Kå€¼åˆ—è¡¨

        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        # æ£€ç´¢æ–‡æ¡£
        retrieved_ids, _ = self.retrieve_documents(query, k=max(k_values))

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        return self.compute_retrieval_metrics(
            retrieved_ids=retrieved_ids,
            relevant_docs=relevant_docs,
            k_values=k_values
        )

    def evaluate_single_question(
        self,
        question: str,
        ground_truth: str,
        relevant_docs: List[str],
        k: int = 3
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªé—®é¢˜ï¼ˆæ£€ç´¢ + ç«¯åˆ°ç«¯ï¼‰
        æ³¨æ„: è¿™é‡Œä¸åŒ…æ‹¬ragasç”Ÿæˆè¯„ä¼°,å› ä¸ºéœ€è¦å®é™…çš„LLMç­”æ¡ˆ

        Args:
            question: é—®é¢˜
            ground_truth: æ ‡å‡†ç­”æ¡ˆ
            relevant_docs: æ ‡æ³¨çš„ç›¸å…³æ–‡æ¡£ID
            k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # 1. æ£€ç´¢è¯„ä¼°
        start_time = time.time()
        retrieved_ids, contexts = self.retrieve_documents(question, k=k)
        retrieval_time = time.time() - start_time

        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        retrieval_metrics = self.compute_retrieval_metrics(
            retrieved_ids=retrieved_ids,
            relevant_docs=relevant_docs,
            k_values=[3, 5]
        )

        # 2. ç«¯åˆ°ç«¯æŒ‡æ ‡
        end_to_end_metrics = {
            'retrieval_time': retrieval_time,
            'num_retrieved': len(retrieved_ids)
        }

        return {
            'question': question,
            'retrieved_ids': retrieved_ids,
            'contexts': contexts,  # ç”¨äºåç»­ragasè¯„ä¼°
            'retrieval_metrics': retrieval_metrics,
            'end_to_end_metrics': end_to_end_metrics
        }

    def evaluate_dataset(
        self,
        test_data_path: str,
        output_path: str = None
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ•´ä¸ªæµ‹è¯•æ•°æ®é›†

        Args:
            test_data_path: æµ‹è¯•æ•°æ®é›†è·¯å¾„(JSONLæ ¼å¼)
            output_path: ç»“æœä¿å­˜è·¯å¾„(å¯é€‰)

        Returns:
            å¹³å‡æŒ‡æ ‡å­—å…¸
        """
        print(f"\nğŸ“Š å¼€å§‹è¯„ä¼°æ•°æ®é›†: {test_data_path}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = []
        with open(test_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                test_data.append(json.loads(line))

        print(f"å…±åŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")

        # è¯„ä¼°æ¯ä¸ªé—®é¢˜
        all_results = []
        all_metrics = {
            'recall@3': [],
            'recall@5': [],
            'precision@3': [],
            'mrr': [],
            'retrieval_time': []
        }

        for i, item in enumerate(test_data, 1):
            print(f"\nå¤„ç†é—®é¢˜ {i}/{len(test_data)}: {item['question'][:50]}...")

            result = self.evaluate_single_question(
                question=item['question'],
                ground_truth=item.get('answer', ''),
                relevant_docs=item.get('relevant_docs', [])
            )

            all_results.append(result)

            # æ”¶é›†æŒ‡æ ‡
            for metric_name in ['recall@3', 'recall@5', 'precision@3', 'mrr']:
                all_metrics[metric_name].append(
                    result['retrieval_metrics'][metric_name]
                )
            all_metrics['retrieval_time'].append(
                result['end_to_end_metrics']['retrieval_time']
            )

            # æ‰“å°å½“å‰ç»“æœ
            print(f"  Recall@3: {result['retrieval_metrics']['recall@3']:.3f}")
            print(f"  MRR: {result['retrieval_metrics']['mrr']:.3f}")

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {
            metric_name: sum(values) / len(values)
            for metric_name, values in all_metrics.items()
        }

        # æ‰“å°æ€»ç»“
        print("\n" + "="*50)
        print("ğŸ“ˆ è¯„ä¼°ç»“æœæ€»ç»“")
        print("="*50)
        for metric_name, value in avg_metrics.items():
            print(f"{metric_name:20s}: {value:.4f}")

        # ä¿å­˜ç»“æœ
        if output_path:
            self._save_results(all_results, avg_metrics, output_path)

        return avg_metrics

    def _save_results(
        self,
        all_results: List[Dict],
        avg_metrics: Dict[str, float],
        output_path: str
    ):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
        output_data = {
            'summary': avg_metrics,
            'details': all_results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


# ========================================
# ç¬¬ä¸‰éƒ¨åˆ†: Ragasè¯„ä¼°(éœ€è¦LLMç”Ÿæˆç­”æ¡ˆ)
# ========================================

class RagasEvaluator:
    """Ragasç”Ÿæˆè´¨é‡è¯„ä¼°"""

    @staticmethod
    def prepare_ragas_dataset(evaluation_results: List[Dict]) -> Dataset:
        """
        å°†è¯„ä¼°ç»“æœè½¬æ¢ä¸ºragaséœ€è¦çš„æ ¼å¼

        æ³¨æ„: è¿™é‡Œå‡è®¾evaluation_resultsä¸­å·²ç»åŒ…å«äº†LLMç”Ÿæˆçš„ç­”æ¡ˆ
        """
        ragas_data = {
            'question': [],
            'answer': [],  # LLMç”Ÿæˆçš„ç­”æ¡ˆ
            'contexts': [],
            'ground_truth': []
        }

        for result in evaluation_results:
            ragas_data['question'].append(result['question'])
            ragas_data['answer'].append(result.get('generated_answer', ''))  # éœ€è¦å…ˆç”Ÿæˆ
            ragas_data['contexts'].append(result['contexts'])
            ragas_data['ground_truth'].append(result.get('ground_truth', ''))

        return Dataset.from_dict(ragas_data)

    @staticmethod
    def evaluate_with_ragas(dataset: Dataset) -> Dict[str, float]:
        """
        ä½¿ç”¨ragasè¯„ä¼°ç”Ÿæˆè´¨é‡

        æ³¨æ„: éœ€è¦å…ˆç”Ÿæˆç­”æ¡ˆ,è¿™é‡Œåªæ˜¯å±•ç¤ºå¦‚ä½•è°ƒç”¨ragas
        """
        print("\nğŸ¤– å¼€å§‹Ragasè¯„ä¼°...")

        result = evaluate(
            dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall
            ]
        )

        return result


# ========================================
# ç¬¬å››éƒ¨åˆ†: ä¸»å‡½æ•°ç¤ºä¾‹
# ========================================

def main():
    """ä¸»è¯„ä¼°æµç¨‹"""

    # é…ç½®è·¯å¾„
    # âš ï¸  æ³¨æ„ï¼šè¿™äº›æ˜¯å…¶ä»–é¡¹ç›®ï¼ˆæ¯•ä¸šè®¾è®¡ï¼‰çš„è·¯å¾„ï¼Œè¯·æ ¹æ®å®é™…é¡¹ç›®ä¿®æ”¹
    # å»ºè®®ä½¿ç”¨ config.path_config ä¸­çš„è·¯å¾„é…ç½®
    VECTOR_STORE_PATH = "/Users/salutethedawn/Desktop/æ¯•ä¸šè®¾è®¡/data/vector_stores/python_tutorial_knowledge_base"
    TEST_DATA_PATH = "/Users/salutethedawn/Desktop/æ¯•ä¸šè®¾è®¡/data/test_dataset.jsonl"
    OUTPUT_PATH = "/Users/salutethedawn/Desktop/æ¯•ä¸šè®¾è®¡/results/evaluation_results.json"

    # å½“å‰é¡¹ç›®çš„æ¨èé…ç½®ï¼ˆå–æ¶ˆæ³¨é‡Šä½¿ç”¨ï¼‰ï¼š
    # from config.path_config import PRIVATE_KB_VECTOR, TEST_DATASET_PATH, EVALUATION_RESULTS_DIR
    # VECTOR_STORE_PATH = str(PRIVATE_KB_VECTOR)
    # TEST_DATA_PATH = str(TEST_DATASET_PATH)
    # OUTPUT_PATH = str(EVALUATION_RESULTS_DIR / "evaluation_results.json")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CompleteEvaluator(
        vector_store_path=VECTOR_STORE_PATH,
        embedding_model_name="BAAI/bge-small-zh-v1.5"
    )

    # æ–¹å¼1: è¯„ä¼°å•ä¸ªé—®é¢˜
    print("\n" + "="*50)
    print("ç¤ºä¾‹1: è¯„ä¼°å•ä¸ªé—®é¢˜")
    print("="*50)

    single_result = evaluator.evaluate_single_question(
        question="ä»€ä¹ˆæ˜¯è½¯ä»¶æµ‹è¯•?",
        ground_truth="è½¯ä»¶æµ‹è¯•æ˜¯...",
        relevant_docs=["15_è½¯ä»¶æµ‹è¯•åŸºç¡€_s1_c0", "15_è½¯ä»¶æµ‹è¯•åŸºç¡€_s1_c1"]
    )

    print(f"æ£€ç´¢åˆ°çš„æ–‡æ¡£: {single_result['retrieved_ids'][:3]}")
    print(f"Recall@3: {single_result['retrieval_metrics']['recall@3']:.3f}")

    # æ–¹å¼2: è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
    print("\n" + "="*50)
    print("ç¤ºä¾‹2: è¯„ä¼°æ•´ä¸ªæ•°æ®é›†")
    print("="*50)

    avg_metrics = evaluator.evaluate_dataset(
        test_data_path=TEST_DATA_PATH,
        output_path=OUTPUT_PATH
    )

    print("\nâœ… è¯„ä¼°å®Œæˆ!")

    # æ–¹å¼3: Ragasè¯„ä¼°(éœ€è¦å…ˆç”Ÿæˆç­”æ¡ˆ)
    # è¿™é‡Œåªæ˜¯ç¤ºä¾‹,å®é™…éœ€è¦å…ˆè°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
    print("\n" + "="*50)
    print("æç¤º: Ragasè¯„ä¼°éœ€è¦å…ˆç”Ÿæˆç­”æ¡ˆ")
    print("="*50)
    print("1. ä½¿ç”¨evaluator.retrieve_documents()è·å–contexts")
    print("2. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ")
    print("3. ä½¿ç”¨RagasEvaluator.evaluate_with_ragas()è¯„ä¼°")


if __name__ == "__main__":
    main()
